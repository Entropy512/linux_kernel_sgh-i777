                                                             -*- org -*-

This document covers how to use the Virtual Contiguous Memory framework
(VCM), how the implementation works, and how to implement MMU drivers
that can be plugged into VCM.  It also contains a rationale for VCM.

* The Virtual Contiguous Memory Manager

The VCM was built to solve the system-wide memory mapping issues that
occur when many bus-masters have IOMMUs.

An IOMMU maps device addresses to physical addresses.  It also
insulates the system from spurious or malicious device bus
transactions and allows fine-grained mapping attribute control.  The
Linux kernel core does not contain a generic API to handle IOMMU
mapped memory; device driver writers must implement device specific
code to interoperate with the Linux kernel core.  As the number of
IOMMUs increases, coordinating the many address spaces mapped by all
discrete IOMMUs becomes difficult without in-kernel support.

The VCM API enables device independent IOMMU control, virtual memory
manager (VMM) interoperation and non-IOMMU enabled device
interoperation by treating devices with or without IOMMUs and all CPUs
with or without MMUs, their mapping contexts and their mappings using
common abstractions.  Physical hardware is given a generic device type
and mapping contexts are abstracted into Virtual Contiguous Memory
(VCM) regions.  Users "reserve" memory from VCMs and "bind" their
reservations with physical memory.

If drivers limit their use of VCM contexts to a some subset of VCM
functionality, they can work with no changes with or without MMU.

** Why the VCM is Needed

Driver writers who control devices with IOMMUs must contend with
device control and memory management.  Driver writers have a large
device driver API that they can leverage to control their devices, but
they are lacking a unified API to help them program mappings into
IOMMUs and share those mappings with other devices and CPUs in the
system.

Sharing is complicated by Linux's CPU-centric VMM.  The CPU-centric
model generally makes sense because average hardware only contains
a MMU for the CPU and possibly a graphics MMU.  If every device in the
system has one or more MMUs the CPU-centric memory management (MM)
programming model breaks down.

Abstracting IOMMU device programming into a common API has already
begun in the Linux kernel.  It was built to abstract the difference
between AMD and Intel IOMMUs to support x86 virtualization on both
platforms.  The interface is listed in include/linux/iommu.h.  It
contains interfaces for mapping and unmapping as well as domain
management.  This interface has not gained widespread use outside the
x86; PA-RISC, Alpha and SPARC architectures and ARM and PowerPC
platforms all use their own mapping modules to control their IOMMUs.
The VCM contains an IOMMU programming layer, but since its
abstraction supports map management independent of device control, the
layer is not used directly.  This higher-level view enables a new
kernel service, not just an IOMMU interoperation layer.

** The General Idea: Map Management using Graphs

Looking at mapping from a system-wide perspective reveals a general
graph problem.  The VCM's API is built to manage the general mapping
graph.  Each node that talks to memory, either through an MMU or
directly (physically mapped) can be thought of as the device-end of
a mapping edge.  The other edge is the physical memory (or
intermediate virtual space) that is mapped.  The figure below shows
an example three with CPU and a few devices connected to the memory
directly or through a MMU.

+--------------------------------------------------------------------+
|                               Memory                               |
+--------------------------------------------------------------------+
                                  |
   +------------------+-----------+-------+----------+-----------+
   |                  |                   |          |           |
+-----+            +-----+             +-----+  +--------+  +--------+
| MMU |            | MMU |             | MMU |  | Device |  | Device |
+-----+            +-----+             +-----+  +--------+  +--------+
   |                  |                   |
+-----+       +-------+---+-----....   +-----+
| CPU |       |           |            | GPU |
+-----+  +--------+  +--------+        +-----+
         | Device |  | Device |  ...
         +--------+  +--------+

For each MMU in the system a VCM context is created through an through
which drivers can make reservations and bind virtual addresses to
physical space.  In the direct-mapped case the device is assigned
a one-to-one MMU (as shown on the figure below). This scheme allows
direct mapped devices to participate in general graph management.

+--------------------------------------------------------------------+
|                               Memory                               |
+--------------------------------------------------------------------+
                                  |
   +------------------+-----------+-------+----------------+
   |                  |                   |                |
+-----+            +-----+             +-----+      +------------+
| MMU |            | MMU |             | MMU |      | One-to-One |
+-----+            +-----+             +-----+      +------------+
   |                  |                   |                |
+-----+       +-------+---+-----....   +-----+       +-----+-----+
| CPU |       |           |            | GPU |       |           |
+-----+  +--------+  +--------+        +-----+  +--------+  +--------+
         | Device |  | Device |  ...            | Device |  | Device |
         +--------+  +--------+                 +--------+  +--------+

The CPU nodes can also be brought under the same mapping abstraction
with the use of a light overlay on the existing VMM. This light
overlay allows VCM-managed mappings to interoperate with the common
API.  The light overlay enables this without substantial modifications
to the existing VMM.

In addition to CPU nodes that are running Linux (and the VMM), remote
CPU nodes that may be running other operating systems can be brought
into the general abstraction.  Routing all memory management requests
from a remote node through the central memory management framework
enables new features like system-wide memory migration.  This feature
may only be feasible for large buffers that are managed outside of the
fast-path, but having remote allocation in a system enables features
that are impossible to build without it.

The fundamental objects that support graph-based map management are:
Virtual Contiguous Memory contexts, reservations, and physical memory
allocations.

* Usage Overview

In a nutshell, platform initialises VCM context for each MMU on the
system and possibly one-to-one VCM contexts which are passed to device
drivers.  Later on, drivers make reservation of virtual address space
from the VCM context.  At this point no physical memory has been
committed to the reservation.  To bind physical memory with a
reservation, physical memory is allocated (possibly discontiguous) and
then bound to the reservation.

Single physical allocation can be bound to several different
reservations also from different VCM contexts.  This allows for
devices connected through different MMUs (or directly) to the memory
banks to share physical memory buffers; this also lets it possible to
map such memory into CPU's address space (be it kernel or user space)
so that the same data can be accessed by the CPU.

[[file:../include/linux/vcm.h][include/linux/vcm.h]] includes comments documenting each API.

** Virtual Contiguous Memory context

A Virtual Contiguous Memory context (VCM) abstracts an address space
a device sees.  A VCM is created with a VCM driver dependent call.  It
is destroyed with a call to:

        void vcm_destroy(struct vcm *vcm);

The newly created VCM instance can be passed to any function that needs to
operate on or with a virtual contiguous memory region.  All internals
of the VCM driver and how the mappings are handled is hidden and VCM
driver dependent.

** Bindings

If all that driver needs is allocate some physical space and map it
into its address space, a vcm_make_binding() call can be used:

	struct vcm_res	*__must_check
	vcm_make_binding(struct vcm *vcm, resource_size_t size,
			 unsigned alloc_flags, unsigned res_flags);

This call allocates physical memory, reserves virtual address space
and binds those together.  If all those succeeds a reservation is
returned which has physical memory associated with it.

If driver does not require more complicated VCM functionality, it is
desirable to use this function since it will work on both real MMUs
and one-to-one mappings.

To destroy created binding, vcm_destroy_binding() can be used:

        void vcm_destroy_binding(struct vcm_res *res);

** Physical memory

Physical memory allocations are handled using the following functions:

	struct vcm_phys *__must_check
	vcm_alloc(struct vcm *vcm, resource_size_t size, unsigned flags);

	void vcm_free(struct vcm_phys *phys);

It is noteworthy that physical space allocation is done in the context
of a VCM.  This is especially important in case of one-to-one VCM
contexts which cannot handle discontiguous physical memory.

Also, depending on VCM context, the physical space may be allocated in
parts of different sizes.  For instance, if a given MMU supports
16MiB, 1MiB, 64KiB and 4KiB pages, it is likely that vcm_alloc() in
context of this MMU's driver will try to split into as few as possible
parts of those sizes.

In case of one-to-one VCM contexts, a physical memory allocated with
the call to vcm_alloc() may be usable only with vcm_map() function.

** Mappings

The easiest way to map a physical space into virtual address space
represented by VCM context is to use the vcm_map() function:

	struct vcm_res *__must_check
	vcm_map(struct vcm *vcm, struct vcm_phys *phys, unsigned flags);

This functions reserves address space from VCM context and binds
physical space to it.  To reverse the process vcm_unmap() can be used:

	void vcm_unmap(struct vcm_res *res);

Similarly to vcm_make_binding(), Usage vcm_map() may be advantageous
over the use of vcm_reserve() followed by vcm_bind().  This is not
only true for one-to-one mapping but if it so happens that the call to
vcm_map() request mapping of a physically contiguous space into kernel
space, a direct mapping can be returned instead of creating a new one.

In some cases, a reservation created with vcm_map() can be used only
with the physical memory passed as the argument to vcm_map() (so if
user chooses to call vcm_unbind() and then vcm_bind() on a different
physical memory, the call may fail).

** Reservations

A reservation is a contiguous region allocated from a virtual address
space represented by VCM context.  Just after reservation is created,
no physical memory needs to be is bound to it.  To manage reservations
following two functions are provided:

	struct vcm_res *__must_check
	vcm_reserve(struct vcm *vcm, resource_size_t size,
		    unsigned flags);

	void vcm_unreserve(struct vcm_res *res);

The first one creates a reservation of desired size, and the second
one destroys it.

** Binding memory

To bind a physical memory into a reservation vcm_bind() function is
used:

	int __must_check vcm_bind(struct vcm_res *res,
				  struct vcm_phys *phys);

When the binding is no longer needed, vcm_unbind() destroys the
connection:

	struct vcm_phys *vcm_unbind(struct vcm_res *res);

** Activating mappings

Unless a VCM context is activated, none of the bindings are actually
guaranteed to be available.  When device driver needs the mappings
it need to call vcm_activate() function to guarantee that the mappings
are sent to hardware MMU.

	int  __must_check vcm_activate(struct vcm *vcm);

After VCM context is activated all further bindings (made with
vcm_make_binding(), vcm_map() or vcm_bind()) will be updated so there
is no need to call vcm_activate() after each binding is done or
undone.

To deactivate the VCM context vcm_deactivate() function is used:

	void vcm_deactivate(struct vcm *vcm);

Both of those functions can be called several times if all calls to
vcm_activate() are paired with a later call to vcm_deactivate().

** Aquiring and releasing ownership of a reservation

Once a device driver reserve a reservation, it may want to pass other device
drivers or attach the reservation in a data structre. Since the reservation
may be shared among many device drivers, the VCM context is needed to provide
a simple way to unreserve a reservation.

Below 2 functions gives the ownership of a reservation to the caller:

	struct vcm_res *__must_check
	vcm_reserve(struct vcm *vcm, resource_size_t size, unsigned flags);

	int __must_check vcm_ref_reserve(struct vcm_res *res);

vcm_reserve() creates a new reservation, thus the first owner of the
reservation is set to the caller of vcm_reservation(). It then passes to a
function the reservation. The function that received the reservation calls
vcm_ref_reserve() to acquire the ownership of the given reservation. If the
function decides that it does not need the reservation any more, it calls
vcm_release() to release the ownership of the reservation.

	void vcm_unreserve(struct vcm_res *res);

It is not required to determine if other functions and drivers still need to
access the reservation because this function just release the ownership of the
reservation. If vcm_unreserve() finds no one has the ownership of the given
reservation, only then does it unreserve (remove) the given reservation.

** Device driver example

The following is a simple, untested example of how platform and
devices work together to use the VCM framework.  Platform initialises
contexts for each MMU in the systems, and through platform device data
passes them to correct drivers.

Device driver header file:

	struct foo_platform_data {
		/* ... */
		struct vcm	*vcm;
		/* ... */
	};

Platform code:

	static int plat_bar_vcm_init(void)
	{
		struct foo_platform_data *fpdata;
		struct vcm *vcm;

		vcm = vcm_baz_create(...);
		if (IS_ERR(vcm))
			return PTR_ERR(vcm);

		fpdata = dev_get_platdata(&foo_device.dev);
		fpdata->vcm = vcm;

		/* ... */

		return 0;
	}

Device driver implementation:

	struct foo_private {
		/* ... */
		struct vcm_res	*fw;
		/* ... */
	};

	static inline struct vcm_res *__must_check
	__foo_alloc(struct device *dev, size_t size)
	{
		struct foo_platform_data *pdata =
			dev_get_platdata(dev);
		return vcm_make_binding(pdata->vcm, size, 0, 0);
	}

	static inline void __foo_free(struct vcm_res *res)
	{
		vcm_destroy_binding(res);
	}

	static int foo_probe(struct device *dev)
	{
		struct foo_platform_data *pdata =
			dev_get_platdata(dev);
		struct foo_private *priv;

		if (IS_ERR_OR_NULL(pdata->vcm))
			return pdata->vcm ? PTR_ERR(pdata->vcm) : -EINVAL;

		priv = kzalloc(sizeof *priv, GFP_KERNEL);
		if (!priv)
			return -ENOMEM;

		/* ... */

		priv->fw = __foo_alloc(dev, 1 << 20);
		if (IS_ERR(priv->fw)) {
			kfree(priv);
			return PTR_ERR(priv->fw);
		}
		/* copy firmware to fw */

		vcm_activate(pdata->vcm);

		dev->p = priv;

		return 0;
	}

	static int foo_remove(struct device *dev)
	{
		struct foo_platform_data *pdata =
			dev_get_platdata(dev);
		struct foo_private *priv = dev->p;

		/* ... */

		vcm_deactivate(pdata->vcm);
		__foo_free(priv->fw);

		kfree(priv);

		return 0;
	}

	static int foo_do_something(struct device *dev, /* ... */)
	{
		struct foo_platform_data *pdata =
			dev_get_platdata(dev);
		struct vcm_res *buf;
		int ret;

		buf = __foo_alloc(/* ... size ...*/);
		if (IS_ERR(buf))
			return ERR_PTR(buf);

		/*
		 * buf->start is address visible from device's
		 * perspective.
		 */

		/* ... set hardware up ... */

		/* ... wait for completion ... */

		__foo_free(buf);

		return ret;
	}

In the above example only vcm_make_binding() function is used so that
the above scheme will work not only for systems with MMU but also in
case of one-to-one VCM context.

** IOMMU and one-to-one contexts

The following example demonstrates mapping IOMMU and one-to-one
reservations to the same physical memory.  For readability, error
handling is not shown on the listings.

First, each contexts needs to be created.  A call used for creating
context is dependent on the driver used.  The following is just an
example of how this could look like:

	struct vcm *vcm_onetoone, *vcm_iommu;

	vcm_onetoone = vcm_onetoone_create();
	vcm_iommu    = vcm_foo_mmu_create();

Once contexts are created, physical space needs to be allocated,
reservations made on each context and physical memory mapped to those
reservations.  Because there is a one-to-one context, the memory has
to be allocated from its context.  It's also best to map the memory in
the single call using vcm_make_binding():

	struct vcm_res *res_onetoone;

	res_onetoone = vcm_make_binding(vcm_o2o, SZ_2MB | SZ_4K, 0, 0);

What's left is map the space in the other context.  If the reservation
in the other two contexts won't be used for any other purpose then to
reference the memory allocated in above, it's best to use vcm_map():

	struct vcm_res *res_iommu;

	res_iommu = vcm_map(vcm_iommu, res_onetoone->phys, 0);

Once the bindings have been created, the contexts need to be activated
to make sure that they are actually on the hardware. (In case of
one-to-one mapping it's most likely a no-operation but it's still
required by the VCM API so it must not be omitted.)

	vcm_activate(vcm_onetoone);
	vcm_activate(vcm_iommu);

At this point, both reservations represent addresses in respective
address space that is bound to the same physical memory.  Devices
connected through the MMU can access it, as well as devices connected
directly to the memory banks.  The bus address for the devices and
virtual address for the CPU is available through the 'start' member of
the vcm_res structure (ie. res_* objects above).

Once the mapping is no longer used and memory no longer needed it can
be freed as follows:

	vcm_unmap(res_iommu);
	vcm_destroy_binding(res_onetoone);

If the contexts are not needed either, they can be disabled:

	vcm_deactivate(vcm_iommu);
	vcm_deactivate(vcm_onetoone);

and than, even destroyed:

	vcm_destroy(vcm_iommu);
	vcm_destroy(vcm_onetoone);

* Available drivers

Not all drivers support all of the VCM functionality.  What is always
supported is:

	vcm_free()
	vcm_unbind()
	vcm_unreserve()

Even though, vcm_unbind() may leave virtual reservation in unusable
state.

The following VCM drivers are provided:

** Virtual Memory Manager driver

Virtual Memory Manager driver is available as vcm_vmm and lets one map
VCM managed physical memory into kernel space.  The calls that this
driver supports are:

	vcm_make_binding()
	vcm_destroy_binding()

	vcm_alloc()

	vcm_map()
	vcm_unmap()

vcm_map() is likely to work with physical memory allocated in context
of other drivers as well (the only requirement is that "page" field of
struct vcm_phys_part will be set for all physically contiguous parts
and that each part's size will be multiply of PAGE_SIZE).

** Real hardware drivers

There are no real hardware drivers at this time.

** One-to-One drivers

As it has been noted, one-to-One drivers are limited in the sense that
certain operations are very unlikely to succeed.  In fact, it is often
certain that some operations will fail.  If your driver needs to be
able to run with One-to-One driver you should limit operations to:

	vcm_make_binding()
	vcm_destroy_binding()

unless the above are not enough, then the following two may be used as
well:

	vcm_map()
	vcm_unmap()

If one uses vcm_unbind() then vcm_bind() on the same reservation,
physical memory pair should also work.

There are no One-to-One drivers at this time.

* Writing a VCM driver

The core of VCM does not handle communication with the MMU.  For this
purpose a VCM driver is used.  Its purpose is to manage virtual
address space reservations, physical allocations as well as updating
mappings in the hardware MMU.

API designed for VCM drivers is described in the
[[file:../include/linux/vcm-drv.h][include/linux/vcm-drv.h]] file so it might be a good idea to take a look
inside.

VCM provides API for three different kinds of drivers.  The most
basic is a core VCM which VCM use directly.  Other then that, VCM
provides two wrappers -- VCM MMU and VCM One-to-One -- which can be
used to create drivers for real hardware VCM contexts and for
One-to-One contexts.

All of the drivers need to provide a context creation functions which
will allocate memory, fill start address, size and pointer to driver
operations, and then call an init function which fills rest of the
fields and validates entered values.

** Writing a core VCM driver

The core driver needs to provide a context creation function as well
as at least some of the following operations:

	void (*cleanup)(struct vcm *vcm);

	int (*alloc)(struct vcm *vcm, resource_size_t size,
		     struct vcm_phys **phys, unsigned alloc_flags,
		     struct vcm_res **res, unsigned res_flags);
	struct vcm_res *(*res)(struct vcm *vcm, resource_size_t size,
			       unsigned flags);
	struct vcm_phys *(*phys)(struct vcm *vcm, resource_size_t size,
				 unsigned flags);

	void (*unreserve)(struct vcm_res *res);

	struct vcm_res *(*map)(struct vcm *vcm, struct vcm_phys *phys,
			       unsigned flags);
	int (*bind)(struct vcm_res *res, struct vcm_phys *phys);
	void (*unbind)(struct vcm_res *res);

	int (*activate)(struct vcm *vcm);
	void (*deactivate)(struct vcm *vcm);

All of the operations (expect for the alloc) may assume that all
pointer arguments are not-NULL.  (In case of alloc, if any argument is
NULL it is either phys or res (never both).)

*** Context creation

To use a VCM driver a VCM context has to be provided which is bound to
the driver.  This is done by a driver-dependent call defined in it's
header file.  Such a call may take varyous arguments to configure the
context of the MMU.  Its prototype may look as follows:

	struct vcm *__must_check vcm_samp_create(/* ... */);

The driver will most likely define a structure encapsulating the vcm
structure (in the usual way).  The context creation function must
allocate space for such a structure and initialise it correctly
including all members of the vcm structure expect for activations.
The activations member is initialised by calling:

	struct vcm *__must_check vcm_init(struct vcm *vcm);

This function also validates that all fields are set correctly.

The driver field of the vcm structure must point to a structure with
all operations supported by the driver.

If everything succeeds, the function has to return pointer to the vcm
structure inside the encapsulating structure.  It is the pointer that
will be passed to all of the driver's operations.  On error,
a pointer-error must be returned (ie. not NULL).

The function might look something like the following:

	struct vcm *__must_check vcm_foo_create(/* ... */)
	{
		struct vcm_foo *foo;
		struct vcm *vcm;

		foo = kzalloc(sizeof *foo, GFP_KERNEL);
		if (!foo)
			return ERR_PTR(-ENOMEM);

		/* ... do stuff ... */

		foo->vcm.start  = /* ... */;
		foo->vcm.size   = /* ... */;
		foo->vcm.driver = &vcm_foo_driver;

		vcm = vcm_init(&foo->vcm);
		if (IS_ERR(vcm)) {
			/* ... error recovery ... */
			kfree(foo);
		}
		return vcm;
	}

*** Cleaning up

The cleanup operation is called when the VCM context is destroyed.
Its purpose is to free all resources acquired when VCM context was
created including the space for the context structure.  If it is not
given, the memory is freed using the kfree() function.

*** Allocation and reservations

If alloc operation is specified, res and phys operations are ignored.
The observable behaviour of the alloc operation should mimic as
closely as possible res and phys operations called one after the
other.

The reason for this operation is that in case of one-to-one VCM
contexts, the driver may not be able to bind together arbitrary
reservation with an arbitrary physical space.  In one-to-one contexts,
reservations and physical memory are tight together and need to be
made at the same time to make binding possible.

The alloc operation may be called with both, res and phys being set,
or at most one of them being NULL.

The res operation reserves virtual address space in the VCM context.
The function must set the start and res_size members of the vcm_res
structure -- all other fields are filled by the VCM framework.

The phys operation allocates physical space which can later be bound
to the reservation.

Both phys and alloc callbacks need to provide a free callbakc along
with the vc_phys structure, which will, as one may imagine, free
allocated space when user calls vcm_free().

Unless VCM driver needs some special handling of physical memory, the
vcm_phys_alloc() function can be used:

	struct vcm_phys *__must_check
	vcm_phys_alloc(resource_size_t size, unsigned flags,
		       const unsigned char *orders);

The last argument of this function (orders) is an array of orders of
page sizes that function should try to allocate.  This array must be
sorted from highest order to lowest and the last entry must be zero.

For instance, an array { 8, 4, 0 } means that the function should try
and allocate 1MiB, 64KiB and 4KiB pages (this is assuming PAGE_SIZE is
4KiB which is true for all supported architectures).  For example, if
requested size is 2MiB and 68 KiB, the function will try to allocate
two 1MiB pages, one 64KiB page and one 4KiB page.  This may be useful
when the mapping is written to the MMU since the largest possible
pages will be used reducing the number of entries.

The function allocates memory from DMA32 zone.  If driver has some
other requirements (that is require different GFP flags) it can use
__vcm_phys_alloc() function which, besides arguments that
vcm_phys_alloc() accepts, take GFP flags as the last argument:

	struct vcm_phys *__must_check
	__vcm_phys_alloc(resource_size_t size, unsigned flags,
			 const unsigned char *orders, gfp_t gfp);

However, if those functions are used, VCM driver needs to select an
VCM_PHYS Kconfig option or oterwise they won't be available.

All those operations may assume that size is a non-zero and divisible
by PAGE_SIZE.

*** Binding

The map operation is optional and it joins res and bind operations
together.  Like alloc operation, this is provided because in case of
one-to-one mappings, the VCM driver may be unable to bind together
physical space with an arbitrary reservation.

Moreover, in case of some VCM drivers, a mapping for given physical
memory can already be present (ie. in case of using VMM).

Reservation created with map operation does not have to be usable
with any other physical space then the one provided when reservation
was created.

The bind operation binds given reservation with a given physical
memory.  The operation may assume that reservation given as an
argument is not bound to any physical memory.

Whichever of the two operation is used, the binding must be reflected
on the hardware if the VCM context has been activated.  If VCM context
has not been activated this is not required.

The vcm_map() function uses map operation if one is provided.
Otherwise, it falls back to alloc or res operation followed by bind
operation.  If this is also not possible, -EOPNOTSUPP is returned.
Similarly, vcm_bind() function uses the bind operation unless it is
not provided in which case -EOPNOTSUPP is returned.

Also, if alloc operation is not provided but map is, the
vcm_make_binding() function will use phys and map operations.

*** Freeing resources

The unbind callback removes the binding between reservation and
a physical memory.  If unbind operation is not provided, VCM assumes
that it is a no-operation.

The unreserve callback releases a reservation as well as free
allocated space for the vcm_res structure.  It is required and if it
is not provided vcm_unreserve() will generate a warning.

*** Activation

When VCM context is activated, the activate callback is called.  It is
called only once even if vcm_activate() is called several times on the
same context.

When VCM context is deactivated (that is, if for each call to
vcm_activate(), vcm_deactivate() was called) the deactivate callback
is called.

When VCM context is activated, all bound reservations must be
reflected on the hardware MMU (if any).  Also, ofter activation, all
calls to vcm_bind(), vcm_map() or vcm_make_binding() must
automatically reflect new mappings on the hardware MMU.

Neither of the operations are required and if missing, VCM will
assume they are a no-operation and no warning will be generated.

*** Ownership of a reservation

When to aquire the ownership of a reservation:
  - When creating a new reservation (having the ownership automatically)
  - When assigning a reservation to a member of data structure
  - When a reservation is passed from the caller function
  - When requiring to access a reservation (that is a global variable)
    at first in its context.
The first one is done with vcm_reserve() and others with vcm_ref_reserve().

When to release the ownership of a reservation:
  - When a reservation is no longer needed in its context.
  - When returning from a function and the function received a reservation
    from its caller and acquired the ownership of the reservation.
  - When removing a reservation from a data structure that includes a pointer
    to the reservation
It is not required as well unable to remove the reservation explicitly. The
last call to vcm_unreserve() will cause the reservation to be removed.

** Writing a hardware MMU driver

It may be undesirable to implement all of the operations that are
required to create a usable driver.  In case of hardware MMUs a helper
wrapper driver has been created to make writing real drivers as simple
as possible.

The wrapper implements most of the functionality of the driver leaving
only implementation of the actual talking to the hardware MMU in hands
of programmer.  Reservations managements as general housekeeping is
already there.

Note that to use the VCM MMU wrapper one needs to select the VCM_MMU
Kconfig option or otherwise the wrapper won't be available.

*** Context creation

Similarly to normal drivers, MMU driver needs to provide a context
creation function.  Such a function must provide a vcm_mmu object and
initialise vcm.start, vcm.size and driver fields of the structure.
When this is done, vcm_mmu_init() should be called which will
initialise the rest of the fields and validate entered values:

	struct vcm *__must_check vcm_mmu_init(struct vcm_mmu *mmu);

This is, in fact, very similar to the way standard driver is created.

*** Orders

One of the fields of the vcm_mmu_driver structure is orders.  This is
an array of orders of pages supported by the hardware MMU.  It must be
sorted from largest to smallest and zero terminated.

The order is the logarithm with the base two of the size of supported
page size divided by PAGE_SIZE.  For instance, { 8, 4, 0 } means that
MMU supports 1MiB, 64KiB and 4KiB pages.

*** Operations

The three operations that MMU wrapper driver uses are:

	void (*cleanup)(struct vcm *vcm);

	int (*activate)(struct vcm_res *res, struct vcm_phys *phys);
	void (*deactivate)(struct vcm_res *res, struct vcm_phys *phys);

	int (*activate_page)(dma_addr_t vaddr, dma_addr_t paddr,
			     unsigned order, void *vcm),
	int (*deactivate_page)(dma_addr_t vaddr, dma_addr_t paddr,
			       unsigned order, void *vcm),

The first one frees all resources allocated by the context creation
function (including the structure itself).  If this operation is not
given, kfree() will be called on vcm_mmu structure.

The activate and deactivate operations are required and they are used
to update mappings in the MMU.  Whenever binding is activated or
deactivated the respective operation is called.

To divide mapping into physical pages, vcm_phys_walk() function can be
used:

	int vcm_phys_walk(dma_addr_t vaddr, const struct vcm_phys *phys,
			  const unsigned char *orders,
			  int (*callback)(dma_addr_t vaddr, dma_addr_t paddr,
					  unsigned order, void *priv),
			  int (*recovery)(dma_addr_t vaddr, dma_addr_t paddr,
					  unsigned order, void *priv),
			  void *priv);

It start from given virtual address and tries to divide allocated
physical memory to as few pages as possible where order of each page
is one of the orders specified by orders argument.

It may be easier to implement activate_page and deactivate_page
operations instead thought.  They are called on each individual page
rather then the whole mapping.  It basically incorporates call to the
vcm_phys_walk() function so driver does not need to call it
explicitly.

** Writing a one-to-one VCM driver

Similarly to a wrapper for a real hardware MMU a wrapper for
one-to-one VCM contexts has been created.  It implements all of the
houskeeping operations and leaves only contiguous memory management
(that is allocating and freeing contiguous regions) to the VCM O2O
driver.

As with other drivers, one-to-one driver needs to provide a context
creation function.  It needs to allocate space for vcm_o2o structure
and initialise its vcm.start, vcm.end and driver fields.  Calling
vcm_o2o_init() will fill the other fields and validate entered values:

	struct vcm *__must_check vcm_o2o_init(struct vcm_o2o *o2o);

There are the following two operations used by the wrapper:

	void (*cleanup)(struct vcm *vcm);
	struct vcm_phys *(*phys)(struct vcm *vcm, resource_size_t size,
				 unsigned flags);

The cleanup operation cleans the context and frees all resources.  If
not provided, kfree() is used.

The phys operation is used in the same way as the core driver's phys
operation.  The only difference is that it must return a physically
contiguous memory block -- ie. returned structure must have only one
part.  On error, the operation must return an error-pointer.  It is
required.

Note that to use the VCM one-to-one wrapper one needs to select the
VCM_O2O Kconfig option or otherwise the wrapper won't be available.

* Epilogue

The initial version of the VCM framework was written by Zach Pfeffer
<zpfeffer@codeaurora.org>.  It was then redesigned and mostly
rewritten by Michal Nazarewicz <m.nazarewicz@samsung.com>.

The new version is still lacking a few important features.  Most
notably, no real hardware MMU has been implemented yet.  This may be
ported from original Zach's proposal.
